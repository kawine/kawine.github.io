<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Kawin Ethayarajh</title>

    <!-- Bootstrap -->
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style type="text/css">
      body { 
        font-family: "Helvetica"; 
        margin-top: 5%;
      }

      h5 {
        font-size: 14px; 
        color: grey 
      }

      .top-buffer { margin-top: 40px; }
      .year { margin-top: 30px; margin-bottom: 10px; }
      .bottom-buffer { margin-bottom: 40px; }

      #pubs li { margin: 15px 0px; }

      @media (min-width: 992px) {
        .sidebar {
          position: fixed;
        }

        #portrait {
          width: 18%;
        }
      }

      @media (max-width: 992px) {
        .sidebar {
          padding-bottom: 5%;	
          margin: auto;
          text-align: center;
        }

        #portrait {
          width: 50%;
        }
      }
    </style>
  </head>

  <body>
    <div class="container">
      <div class="col-md-4">
        <div class="row sidebar">
            <img id="portrait" src="assets/new_profile.jpeg">
            <div class="year" style="font-size:19pt; font-weight: 300">
              <b class="name">KAWIN</b> ETHAYARAJH 
            </div>
            <i class="fa fa-fw fa-map-marker"></i> Stanford, CA<br>
            <i class="fa fa-fw fa-envelope"></i> firstname@stanford.edu</a><br>
            <a href="https://twitter.com/ethayarajh"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
            <a href="https://scholar.google.ca/citations?user=7SUV6rQAAAAJ&hl=en"><i class="fa fa-fw fa-google"></i> Google Scholar</a><br>
            <a href="https://github.com/kawine"><i class="fa fa-fw fa-github"></i> Github</a><br>
	          <a href="assets/CV.pdf"><i class="fa fa-fw fa-file"></i> CV</a><br>
          
        </div>
      </div>

      <div class="col-md-8 bottom-buffer">
  
          <div class="col-12">
		<p>
			Iâ€™m a fifth-year PhD student in the Stanford AI Lab (SAIL), where I'm advised by Dan Jurafsky and work on <b>behavior-bound machine learning</b>.
		</p>
		<p>
			Machine learning is not a sterile industrial process; much in the way that it is hardware-bound and software-bound, it is also shaped by the behavior of real-world actors such as workers, firms, and states. 
			By borrowing from fields like economics, my work tries to formalize this behavior and create algorithms, tools, and platforms that are compatible with actual actors, not just idealized ones.
		</p>
		<p>
			Highlights:
			<ul style="list-style-type: circle;" id="contributions">
				<li><a href="https://huggingface.co/datasets/stanfordnlp/SHP">SHP</a>, the first large-scale public dataset of human preferences over text (5M examples in v2.0)</li>
				<li><a href="https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430">Archangel</a>, the largest suite of human feedback-aligned LLMs</li>
				<li><a href="https://dynabench.org/">Dynaboard</a>, an evaluation-as-a-service platform used to host Dynabench, BabyLM, and others</li>
				<li><a href="https://github.com/ContextualAI/HALOs">HALOs</a>, a framework for creating prospect-theoretic losses for alignment</li>
			</ul>
		</p>
		<p>
			I have received an <b>ICML 2022 Outstanding Paper</b> award, a <b>Facebook Fellowship</b>, and an <b>NSERC PGS-D</b> during my PhD.
			Prior to Stanford, I was a National Scholar at the University of Toronto.
		</p>
		<p>
			<b>I am on the job market for 2024.</b>
		</p>
          </div>

        <h4 class="top-buffer">Recent Work (<a href="https://scholar.google.com/citations?user=7SUV6rQAAAAJ&hl=en">full list</a>)</h4>
	  <p>
		<b>Principal-Agent Problems in Data Creation</b>
		Datasets are born of a conflict in incentives between those who pay for data (principals) and those who produce it (agents).
		As a result, they are much simpler than the real-world problems they purport to reflect.
		How can we shrink this gap?
		I work on frameworks for understanding dataset difficulty and use them to create datasets like SHP, the first large-scale dataset of human preferences over text.
	  	SHP is one of the few datasets used by Amazon AWS (for reranking generations), Microsoft Deepspeed Chat (to train LLMs) and Llama-2 (one of the most widely-used LLMs).
	  </p>

	  <ul style='margin-bottom: 25px; list-style: none' id="pubs">
		  <li>  
			<i>Understanding Dataset Difficulty with V-Usable Information.</i><br/>
			<u>Kawin Ethayarajh</u>, Yejin Choi, and Swabha Swayamdipta.<br/>
			ICML 2022 (<b>outstanding paper</b>).<br/>
		    	<a href="https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf"><span class="label label-default">paper</span></a>
		    	<a href="https://twitter.com/ethayarajh/status/1449203922057400329"><span class="label label-info">tweet</span></a>
			<a href="https://github.com/kawine/dataset_difficulty"><span class="label label-success">code</span></a>
	        	<a href="https://proceedings.mlr.press/v162/ethayarajh22a.html"><span class="label label-primary">bib</span></a>
		  </li>

		  <li>
			<i>Stanford Human Preferences (SHP).</i><br/>
			<u>Kawin Ethayarajh</u>, Heidi Zhang, Yizhong Wang, and Dan Jurafsky.<br/>
			(online).<br/>
		    	<a href="https://twitter.com/ethayarajh/status/1628442002454085632?lang=en"><span class="label label-info">tweet</span></a>
			<a href="https://huggingface.co/datasets/stanfordnlp/SHP"><span class="label label-danger">dataset</span></a>
	        	<a href="https://proceedings.mlr.press/v162/ethayarajh22a.html"><span class="label label-primary">bib</span></a>  
		  </li>
	  </ul>

	  <p>
	  	<b>Pluralistic Model Alignment</b>
		I discovered that methods for aligning LLMs with human feedback (e.g., RLHF, DPO) work in part because they implicitly model human biases in decision-making.
		This means that there is not one single objective for alignment, but rather a family of <i>human-aware losses (HALOs)</i>.
		I then designed an alignment objective based on Kahneman & Tversky's prospect theory called KTO, which can align LLMs using binary feedback alone.
		KTO is thus far easier to use in the real world, where preferences are scarce and expensive to collect.
	  </p>

	  <ul style='margin-bottom: 25px; list-style: none' id="pubs">
	 	<li>	
			<i>Model Alignment as Prospect Theoretic Optimization.</i><br/>
			<u>Kawin Ethayarajh</u>, Winnie Xu, Muennighoff, Dan Jurafsky, and Douwe Kiela.<br/>
		      	preprint.<br/>
		    	<a href="https://arxiv.org/abs/2402.01306"><span class="label label-default">paper</span></a>
		    	<a href="https://twitter.com/ethayarajh/status/1593028231707643904"><span class="label label-info">tweet</span></a>
			<a href="https://github.com/ContextualAI/HALOs"><span class="label label-success">code</span></a>
		</li>
	  </ul>

	  <p>
          	<b>Utility-Driven Evaluation</b>
		Organizations care not only about the upside from using a model but also its costs (fairness, memory, etc.), which are often ignored in a research setting.
          	Working with researchers at Meta, we developed Dynaboard, a holistic evaluation-as-a-service platform for hosting benchmarks.
          	Dynaboard has been used to host many challenges, including DADC (Dynamic Adversarial Data Collection), DataPerf, BabyLM, and Flores.
	  	The concept of utility-driven evaluation has since gained wide acceptance and underlies many benchmarks, like Stanford's <a href="https://crfm.stanford.edu/helm/latest/">HELM</a>.
	  </p>

	  <ul style='margin-bottom: 25px; list-style: none' id="pubs">
	  	<li>
			<i>Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking.</i><br/>
			Zhiyi Ma<sup>*</sup>, <u>Kawin Ethayarajh</u><sup>*</sup>, Tristan Thrush<sup>*</sup>, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, Douwe Kiela. (*= equal contribution)<br/>
		      	NeurIPS 2021. <br/>
		    	<a href="https://proceedings.neurips.cc/paper/2021/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html"><span class="label label-default">paper</span></a>
			<a href="https://twitter.com/douwekiela/status/1396878157941145600"><span class="label label-info">tweet</span></a>
			<a href="https://dynabench.org/"><span class="label label-success">demo</span></a>
			<a href="https://venturebeat.com/2021/05/24/facebooks-dynabench-now-scores-nlp-models-for-metrics-like-fairness/"><span class="label label-danger">news</span></a> 
			<a href="https://proceedings.neurips.cc/paper/2021/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html"><span class="label label-primary">bib</span></a>		 
		</li>
		  
		<li>
			<i>Utility is in the Eye of the User: A Critique of NLP Leaderboards.</i><br/>
	            	<u>Kawin Ethayarajh</u> and Dan Jurafsky.<br/>
		    	EMNLP 2020.<br/>
		    	<a href="https://www.aclweb.org/anthology/2020.emnlp-main.393/"><span class="label label-default">paper</span></a>
		    	<a href="assets/emnlp2020_leaderboard_slides.pdf"><span class="label label-info">slides</span></a>
	        	<a href="https://www.aclweb.org/anthology/2020.emnlp-main.393.bib"><span class="label label-primary">bib</span></a>
        	</li>
	  </ul>
	<!-- 	

 	<h4 class="top-buffer">Older Work</h4>
		
	   <li>
            	<i>How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.</i><br/>
            	<u>Kawin Ethayarajh</u>.<br/>
	    	EMNLP 2019.<br/>
	    	<a href="https://www.aclweb.org/anthology/D19-1006"><span class="label label-default">paper</span></a>
	    	<a href="https://twitter.com/ethayarajh/status/1225843038099931137"><span class="label label-info">tweet</span></a>
		<a href="https://kawine.github.io/blog/nlp/2020/02/03/contextual.html"><span class="label label-danger">blog</span></a>
		<a href="https://github.com/kawine/contextual"><span class="label label-success">code</span></a>
	    	<a href="https://www.aclweb.org/anthology/D19-1006.bib"><span class="label label-primary">bib</span></a>
          </li>
			
          <li>
          	<i>Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline.</i><br/>
          	<u>Kawin Ethayarajh</u>.<br/>
          	ACL 2018 - Repl4NLP (<b>best paper</b>).<br/>
		<a href="https://www.aclweb.org/anthology/W18-3012"><span class="label label-default">paper</span></a>
		<a href="https://github.com/kawine/usif"><span class="label label-success">code</span></a>
		<a href="https://www.aclweb.org/anthology/W18-3012.bib"><span class="label label-primary">bib</span></a>
          </li> 
		
	  <li>
          	<i>Towards Understanding Linear Word Analogies.</i><br/>
          	<u>Kawin Ethayarajh</u>, David Duvenaud, and Graeme Hirst.<br/>
          	ACL 2019.<br/>
		<a href="https://www.aclweb.org/anthology/P19-1315"><span class="label label-default">paper</span></a>
		<a href="assets/acl_analogies_notes.txt"><span class="label label-default">addendum</span></a>
	    	<a href="https://twitter.com/ethayarajh/status/1142854783377690625"><span class="label label-info">tweet</span></a>
		<a href="https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html"><span class="label label-danger">blog</span></a>
		<a href="https://www.aclweb.org/anthology/P19-1315.bib"><span class="label label-primary">bib</span></a>
          </li>
		-->

        </ul>
      </div>

    </div> <!-- /container -->
  </body>
</html>
