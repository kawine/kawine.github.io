<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Kawin Ethayarajh</title>

    <!-- Bootstrap -->
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style type="text/css">
      body { 
        font-family: "Helvetica"; 
        margin-top: 5%;
      }

      h5 {
        font-size: 14px; 
        color: grey 
      }

      .top-buffer { margin-top: 40px; }
      .year { margin-top: 30px; margin-bottom: 10px; }
      .bottom-buffer { margin-bottom: 40px; }

      #pubs li { margin: 15px 0px; }

      @media (min-width: 992px) {
        .sidebar {
          position: fixed;
        }

        #portrait {
          width: 18%;
        }
      }

      @media (max-width: 992px) {
        .sidebar {
          padding-bottom: 5%;	
          margin: auto;
          text-align: center;
        }

        #portrait {
          width: 50%;
        }
      }
    </style>
  </head>

  <body>
    <div class="container">
      <div class="col-md-4">
        <div class="row sidebar">
            <img id="portrait" src="assets/new_profile.jpeg">
            <div class="year" style="font-size:19pt; font-weight: 300">
              <b class="name">KAWIN</b> ETHAYARAJH 
            </div>
            <i class="fa fa-fw fa-map-marker"></i> Toronto, CA / Princeton, NJ <br>
            <i class="fa fa-fw fa-envelope"></i> ke7953@princeton.edu</a><br>
            <a href="https://twitter.com/ethayarajh"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
            <a href="https://scholar.google.ca/citations?user=7SUV6rQAAAAJ&hl=en"><i class="fa fa-fw fa-google"></i> Google Scholar</a><br>
            <a href="https://github.com/kawine"><i class="fa fa-fw fa-github"></i> Github</a><br>
	          <a href="assets/CV.pdf"><i class="fa fa-fw fa-file"></i> CV</a><br>
          
        </div>
      </div>

      <div class="col-md-8 bottom-buffer">
  
          <div class="col-12">
		<p>
			As of July 2025, I will be an <b>Assistant Professor of Applied AI at UChicago Booth</b>. Currently Iâ€™m a postdoctoral fellow at Princeton Language and Intelligence (PLI), where I work on <b>behavior-bound machine learning</b>.
		</p>
		<p>
			Machine learning is not a sterile industrial process; much in the way that it is hardware-bound and software-bound, it is also shaped by the behavior of real-world actors such as workers, firms, and states. 
			By borrowing from fields like economics, my work tries to formalize this behavior and create algorithms, tools, and platforms that are compatible with actual actors, not just idealized ones.
		</p>
		<p>
			Highlights:
			<ul style="list-style-type: circle;" id="contributions">
				<li><a href="https://huggingface.co/datasets/stanfordnlp/SHP">SHP</a>, the first large-scale public dataset of human preferences over text (5M examples in v2.0)</li>
				<li><a href="https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430">Archangel</a>, the largest suite of human feedback-aligned LLMs</li>
				<li><a href="https://dynabench.org/">Dynaboard</a>, an evaluation-as-a-service platform used to host Dynabench, BabyLM, and others</li>
				<li><a href="https://github.com/ContextualAI/HALOs">HALOs</a>, a framework for creating prospect-theoretic losses for alignment</li>
			</ul>
		</p>
		<p>
			I have received an <b>ICML 2022 Outstanding Paper</b> award, a <b>Facebook Fellowship</b>, and an <b>NSERC PGS-D</b> during my PhD.
			Prior to Princeton, I received a PhD in Computer Science from Stanford, where I was advised by Dan Jurafsky.
		</p>
          </div>

        <h4 class="top-buffer">Recent Work (<a href="https://scholar.google.com/citations?user=7SUV6rQAAAAJ&hl=en">full list</a>)</h4>
	  <p>
		<b>Data</b>
		Datasets are born of a conflict in incentives between those who pay for data (principals) and those who produce it (agents).
		As a result, they are much simpler than the real-world problems they purport to reflect.
		How can we shrink this gap?
		I work on frameworks for understanding dataset difficulty and use them to create datasets like SHP, the first large-scale dataset of human preferences over text.
	  	SHP is one of the few datasets used for the alignment of Llama-2 (one of the most widely-used LLMs).
	  </p>

	  <ul style='margin-bottom: 25px; list-style: none' id="pubs">
		  <li>  
			<i>Understanding Dataset Difficulty with V-Usable Information.</i><br/>
			<u>Kawin Ethayarajh</u>, Yejin Choi, and Swabha Swayamdipta.<br/>
			ICML 2022 (<b>outstanding paper</b> - top 10 of 1233).<br/>
		    	<a href="https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf"><span class="label label-default">paper</span></a>
		    	<a href="https://twitter.com/ethayarajh/status/1449203922057400329"><span class="label label-info">tweet 1</span></a>
			<a href="https://twitter.com/ethayarajh/status/1628442002454085632?lang=en"><span class="label label-info">tweet 2</span></a>
			<a href="https://github.com/kawine/dataset_difficulty"><span class="label label-success">code</span></a>
			<a href="https://huggingface.co/datasets/stanfordnlp/SHP"><span class="label label-danger">dataset</span></a>
	        	<a href="https://proceedings.mlr.press/v162/ethayarajh22a.html"><span class="label label-primary">bib</span></a>
		  </li>
	  </ul>

	  <p>
	  	<b>Alignment</b>
		  Alignment is monolithic: one model aligned with one method on one set of preferences is served to all users.
		  I draw connections between behavioral economics and model alignment so that we can make alignment more pluralistic, discovering for instance, that we can draw from a whole family of <i>human-aware losses (HALOs)</i> instead of just using DPO or PPO.
		  One of these HALOs, called KTO, has become the most popular option for aligning LLMs with unpaired and imbalanced human feedback, the most common type in production settings.
	  </p>

	  <ul style='margin-bottom: 25px; list-style: none' id="pubs">
	 	<li>	
			<i>Model Alignment as Prospect Theoretic Optimization.</i><br/>
			<u>Kawin Ethayarajh</u>, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.<br/>
		      	ICML 2024 (<b>spotlight</b> - top 3.5%).<br/>
		    	<a href="https://arxiv.org/abs/2402.01306"><span class="label label-default">paper</span></a>
		    	<a href="https://x.com/ethayarajh/status/1732837520784957476"><span class="label label-info">tweet</span></a>
			<a href="https://github.com/ContextualAI/HALOs"><span class="label label-success">code</span></a>
		</li>
	  </ul>

	  <p>
          	<b>Evaluation</b>
		  The ML models that researchers consider the best are often not the ones deployed by firms in the real world.
		  But why?
		  The landscape in which models are deployed is heterogeneous, and firms are willing to sacrifice performance for memory efficiency, controllability, and more.
		  I model these tradeoffs and design evaluation pipelines that better simulate real-world considerations, such as Dynaboard.
		  Dynaboard has been used to host DADC (Dynamic Adversarial Data Collection), DataPerf, BabyLM, Flores, and many other challenges.
	  </p>

	  <ul style='margin-bottom: 25px; list-style: none' id="pubs">
	  	<li>
			<i>Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking.</i><br/>
			Zhiyi Ma<sup>*</sup>, <u>Kawin Ethayarajh</u><sup>*</sup>, Tristan Thrush<sup>*</sup>, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, Douwe Kiela. (*= equal contribution)<br/>
		      	NeurIPS 2021. <br/>
		    	<a href="https://proceedings.neurips.cc/paper/2021/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html"><span class="label label-default">paper</span></a>
			<a href="https://twitter.com/douwekiela/status/1396878157941145600"><span class="label label-info">tweet</span></a>
			<a href="https://dynabench.org/"><span class="label label-success">demo</span></a>
			<a href="https://venturebeat.com/2021/05/24/facebooks-dynabench-now-scores-nlp-models-for-metrics-like-fairness/"><span class="label label-danger">news</span></a> 
			<a href="https://proceedings.neurips.cc/paper/2021/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html"><span class="label label-primary">bib</span></a>		 
		</li>
		  
		<li>
			<i>Utility is in the Eye of the User: A Critique of NLP Leaderboards.</i><br/>
	            	<u>Kawin Ethayarajh</u> and Dan Jurafsky.<br/>
		    	EMNLP 2020.<br/>
		    	<a href="https://www.aclweb.org/anthology/2020.emnlp-main.393/"><span class="label label-default">paper</span></a>
			<a href="https://x.com/ethayarajh/status/1311399326413922304"><span class="label label-info">tweet</span></a>
	        	<a href="https://www.aclweb.org/anthology/2020.emnlp-main.393.bib"><span class="label label-primary">bib</span></a>
        	</li>
	  </ul>

        </ul>
      </div>

    </div> <!-- /container -->
  </body>
</html>
